{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from pylab import mpl \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import seaborn as sns \n",
    "import torch.nn as nn \n",
    "import torch \n",
    "from torch.utils.data import Dataset,DataLoader \n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data \n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "from sklearn import metrics \n",
    "import matplotlib  \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df=pd.read_csv('continuous dataset.csv') \n",
    "df['datetime']=pd.to_datetime(df['datetime'])\n",
    "df=df.set_index('datetime') \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#缺失值\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "异常值处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4),dpi=120)\n",
    "sns.lineplot(x=df.index,y=df['nat_demand'])\n",
    "plt.xticks(rotation=30, fontsize=17)# 横坐标倾斜\n",
    "plt.xlabel('date', fontsize=19)\n",
    "plt.yticks(fontsize=17)\n",
    "plt.ylabel('nat_demand', fontsize=19) \n",
    "\n",
    "\n",
    "#plt.xticks(rotation=30, fontsize=10)  \n",
    "#plt.yticks(fontsize=10) \n",
    "#plt.xlabel('Date', fontsize=12)  \n",
    "#plt.ylabel('Demand', fontsize=12)  \n",
    "#plt.title('Natural Gas Demand', fontsize=14) \n",
    "\n",
    "\n",
    "\n",
    "# plt.savefig('nat_demand.jpg',bbox_inches = 'tight',dpi=600) \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制箱线图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除异常值前\n",
    "plt.figure(figsize=(12, 4),dpi=120) # 大小为宽度12英寸，高度4英寸，分辨率为120。\n",
    "sns.boxplot(y=df['nat_demand']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ser=df['nat_demand'] \n",
    "Low = Ser.quantile(0.25)-1.5*(Ser.quantile(0.75)-Ser.quantile(0.25)) \n",
    "Up = Ser.quantile(0.75)+1.5*(Ser.quantile(0.75)-Ser.quantile(0.25)) \n",
    "print(Low,Up) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['nat_demand']=df['nat_demand'].map(lambda x:Up if x>Up else(Low if x<Low else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除异常值后\n",
    "plt.figure(figsize=(12, 4),dpi=120) # 大小为宽度12英寸，高度4英寸，分辨率为120。\n",
    "sns.boxplot(y=df['nat_demand'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.dpi'] = 120 \n",
    "fig = plt.gcf() \n",
    "fig.set_size_inches(15,10)  \n",
    "ax=sns.heatmap(data=df.corr(),\n",
    "            vmax=0.9,  \n",
    "            # cmap=palettable.cmocean.diverging.Curl_10.mpl_colors,\n",
    "            annot=True,  \n",
    "            fmt=\".2f\",  \n",
    "            # annot_kws={'size':8,'weight':'normal', 'color':'#253D24'},\n",
    "            annot_kws={'size':16,'weight':'normal', 'color':'#253D24'},\n",
    "            # mask=np.triu(np.ones_like(data[tmp_list].corr(),dtype=np.bool))\n",
    "            \n",
    "           )\n",
    "#xticklabels = heatmap.get_xticklabels()\n",
    "#yticklabels = heatmap.get_yticklabels()\n",
    "#heatmap.set_xticklabels(xticklabels, fontsize=22)\n",
    "#heatmap.set_yticklabels(yticklabels, fontsize=22)\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=22)\n",
    "\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=22)\n",
    "\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abs(df.corr()['nat_demand']).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除相关性低的变量QV2M_dav\n",
    "df.drop(columns=['QV2M_dav'],axis=1,inplace=True)\n",
    "\n",
    "#person特征选择前5\n",
    "df.drop(columns=['QV2M_san'],axis=1,inplace=True)\n",
    "df.drop(columns=['QV2M_toc'],axis=1,inplace=True)\n",
    "df.drop(columns=['school'],axis=1,inplace=True)\n",
    "df.drop(columns=['TQL_dav'],axis=1,inplace=True)\n",
    "df.drop(columns=['W2M_dav'],axis=1,inplace=True)\n",
    "df.drop(columns=['TQL_toc'],axis=1,inplace=True)\n",
    "df.drop(columns=['W2M_toc'],axis=1,inplace=True)\n",
    "df.drop(columns=['TQL_san'],axis=1,inplace=True)\n",
    "df.drop(columns=['Holiday_ID'],axis=1,inplace=True)\n",
    "df.drop(columns=['holiday'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### shap特征选择前5\n",
    "#df.drop(columns=['school'],axis=1,inplace=True)\n",
    "#df.drop(columns=['holiday'],axis=1,inplace=True)\n",
    "#df.drop(columns=['Holiday_ID'],axis=1,inplace=True)\n",
    "#df.drop(columns=['W2M_toc'],axis=1,inplace=True)\n",
    "#df.drop(columns=['TQL_san'],axis=1,inplace=True)\n",
    "#df.drop(columns=['T2M_toc'],axis=1,inplace=True)\n",
    "#df.drop(columns=['W2M_san'],axis=1,inplace=True)\n",
    "#df.drop(columns=['QV2M_toc'],axis=1,inplace=True)\n",
    "#df.drop(columns=['W2M_dav'],axis=1,inplace=True)\n",
    "#df.drop(columns=['TQL_toc'],axis=1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data, time_step=2, after_day=1, validate_percent=0.8): \n",
    "    seq_length = time_step + after_day \n",
    "    result = []\n",
    "    index_list=[]\n",
    "\n",
    "    for index in range(len(data) - seq_length + 1): \n",
    "        result.append(data[index: index + seq_length])\n",
    "        index_list.append(np.array(data.index)[index: index + seq_length])\n",
    "    result = np.array(result)\n",
    "    index_list = np.array(index_list)\n",
    "    print('total data \\n', result.shape)\n",
    "    train_size = int(len(result) * validate_percent) \n",
    "    train = result[:train_size, :] \n",
    "    validate = result[train_size:, :] \n",
    "\n",
    "    x_train = train[:, :time_step]\n",
    "    y_train = train[:, time_step:][:,:,0]\n",
    "    x_validate = validate[:, :time_step]\n",
    "    y_validate = validate[:, time_step:][:,:,0]#取第一列\n",
    "    \n",
    "    #取index\n",
    "    train_index=index_list[:train_size, :]\n",
    "    validate_index=index_list[train_size:, :]\n",
    "    x_train_index = train_index[:, :time_step]\n",
    "    y_train_index = train_index[:,time_step:]\n",
    "    x_validate_index = validate_index[:, :time_step]\n",
    "    y_validate_index = validate_index[:, time_step:]\n",
    "   \n",
    "    scaler1 = MinMaxScaler(feature_range=(0, 1)) \n",
    "    x_train=scaler1.fit_transform(x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2])).reshape(x_train.shape[0],x_train.shape[1],x_train.shape[2])\n",
    "    x_validate=scaler1.transform(x_validate.reshape(x_validate.shape[0],x_validate.shape[1]*x_validate.shape[2])).reshape(x_validate.shape[0],x_validate.shape[1],x_validate.shape[2])\n",
    "    scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_train=scaler2.fit_transform(y_train)\n",
    "    y_validate=scaler2.transform(y_validate)\n",
    "    return [scaler1,scaler2,result,x_train, y_train, x_validate, y_validate,y_train_index,y_validate_index]\n",
    "time_step=7\n",
    "after_day=1\n",
    "scaler1,scaler2,data,x_train, y_train, x_test, y_test,y_train_index,y_validate_index = load_data(df,time_step=time_step, after_day=after_day, validate_percent=0.8)\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train=x_train[:int(x_train.shape[0]/30)*30]\n",
    "# y_train=y_train[:int(y_train.shape[0]/30)*30]\n",
    "# x_test=x_test[:int(x_test.shape[0]/30)*30]\n",
    "# y_test=y_test[:int(y_test.shape[0]/30)*30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=Variable(torch.Tensor(x_train))\n",
    "x_test=Variable(torch.Tensor(x_test))\n",
    "y_train=Variable(torch.Tensor(y_train))\n",
    "y_test=Variable(torch.Tensor(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据装入\n",
    "def data_generator(x_train,x_test,y_train,y_test,batch_size):\n",
    "    train_dataset=Data.TensorDataset(x_train,y_train)\n",
    "    test_dataset=Data.TensorDataset(x_test,y_test)\n",
    "    train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=False,drop_last=True) \n",
    "    test_loader=torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,drop_last=True)\n",
    "    return train_loader,test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader,test_loader=data_generator(x_train,x_test,y_train,y_test,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BILSTM_Attention(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,num_layers,batch_size):\n",
    "        super().__init__() \n",
    "        self.input_size=input_size \n",
    "        self.hidden_size=hidden_size \n",
    "        self.num_layers=num_layers \n",
    "        self.batch_size=batch_size \n",
    "        self.output_size=output_size \n",
    "        model_name=='BILSTM-ATT':        \n",
    "        self.num_directions = 2 \n",
    "        self.lstm=nn.LSTM(input_size ,hidden_size,num_layers,batch_first=False, bidirectional=True)\n",
    "       \n",
    "             \n",
    "        self.linear1=nn.Linear(self.hidden_size*self.num_directions,self.hidden_size) \n",
    "        self.linear2=nn.Linear(self.hidden_size,output_size) \n",
    "        self.dropout=nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def attention_net(self, lstm_output, final_state): \n",
    "\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        hidden = final_state.view(-1,self.num_directions*self.hidden_size,self.num_layers)  \n",
    "        # print(lstm_output.shape,hidden.shape)\n",
    "        self.attn_weights = torch.bmm(lstm_output, hidden) # attn_weights : [batch_size, n_step]\n",
    "\n",
    "        soft_attn_weights = torch.tanh(self.attn_weights)\n",
    "\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights).squeeze(2)\n",
    "        return context # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "\n",
    "        \n",
    "    def forward(self,input_seq):\n",
    "        input_seq=input_seq.permute(1,0,2)\n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(device)       \n",
    "        model_name=='BILSTM-ATT': \n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq)\n",
    "        # print(lstm_out.shape,self.hidden_cell[0].shape)\n",
    "        attn_output = self.attention_net(lstm_out,self.hidden_cell[0])\n",
    "        # print(attn_output.shape)\n",
    "      \n",
    "        # print(attn_output.view(-1,self.hidden_size*self.num_directions).shape)\n",
    "        out=self.linear1(attn_output.view(-1,self.hidden_size*self.num_directions))\n",
    "        out=torch.tanh(out)\n",
    "        # print(out.shape)\n",
    "        predictions = self.linear2(out)\n",
    "\n",
    "        return predictions\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BiLSTM_BIGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, batch_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.num_layers = num_layers \n",
    "        self.output_size = output_size \n",
    "        self.batch_size = batch_size \n",
    "        model_name=='BILSTM':\n",
    "        self.num_directions = 2\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, self.num_layers, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(self.num_directions * self.hidden_size, self.output_size)\n",
    "       \n",
    "            \n",
    "    def forward(self, input_seq):\n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, self.batch_size, self.hidden_size).to(device)\n",
    "        # print(input_seq.size())\n",
    "        seq_len = input_seq.shape[1] # seq_len = input_seq.shape[1]\n",
    "        model_name=='BILSTM':        \n",
    "        output, _ = self.lstm(input_seq, (h_0, c_0))\n",
    "       \n",
    "        pred = self.linear(output)  # pred()\n",
    "        pred = pred[:, -1, :]     \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "# from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 定义一个类\n",
    "class CNN_BILSTM(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,hidden_size,num_layers,output_size,batch_size,seq_length) -> None:\n",
    "        super(CNN_BILSTM,self).__init__()\n",
    "        self.in_channels=in_channels  \n",
    "        self.out_channels=out_channels \n",
    "        self.hidden_size=hidden_size \n",
    "        self.num_layers=num_layers \n",
    "        self.output_size=output_size \n",
    "        self.batch_size=batch_size \n",
    "        self.seq_length=seq_length \n",
    "        self.num_directions=2 \n",
    "        self.relu = nn.ReLU(inplace=True) \n",
    "        # (batch_size=64, seq_len=3, input_size=3) ---> permute(0, 2, 1)\n",
    "        # (64, 3, 3)\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3), #shape(7,--)  ->(64,3,2)\n",
    "            nn.ReLU())\n",
    "        self.lstm=nn.LSTM(input_size=out_channels,hidden_size=hidden_size,num_layers=num_layers,batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*self.num_directions, output_size)\n",
    "      \n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x.permute(0,2,1)                            \n",
    "        x=self.conv(x) \n",
    "        x=x.permute(0,2,1) \n",
    "        batch_size, seq_len = x.size()[0], x.size()[1] \n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # output(batch_size, seq_len, num_directions * hidden_size)\n",
    "        output, _ = self.lstm(x, (h_0, c_0)) \n",
    "        # print(output.shape)\n",
    "        pred = self.fc(output)   \n",
    "        pred = pred[:, -1, :]  \n",
    "        return pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "# from turtle import forward\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 定义一个类\n",
    "class CNN_BILSTM_Attention(nn.Module): \n",
    "    def __init__(self,in_channels,out_channels,hidden_size,num_layers,output_size,batch_size,seq_length) -> None:\n",
    "        super(CNN_BILSTM_Attention,self).__init__()\n",
    "        self.in_channels=in_channels    \n",
    "        self.out_channels=out_channels\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers \n",
    "        self.output_size=output_size \n",
    "        self.batch_size=batch_size \n",
    "        self.seq_length=seq_length \n",
    "        self.num_directions=2 # 单向LSTM\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # (batch_size=64, seq_len=3, input_size=3) ---> permute(0, 2, 1)\n",
    "        # (64, 3, 3)\n",
    "        self.conv=nn.Sequential(\n",
    "            nn.Conv1d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3), #shape(7,--)  ->(64,3,2)\n",
    "            nn.ReLU()) \n",
    "        self.lstm=nn.LSTM(input_size=out_channels,hidden_size=hidden_size,num_layers=num_layers,batch_first=True,bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size*self.num_directions, output_size)\n",
    "      \n",
    "        \n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        lstm_output = lstm_output.permute(1, 0, 2)\n",
    "        hidden = final_state.view(-1,self.num_directions*self.hidden_size,self.num_layers)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        # print(lstm_output.shape,hidden.shape)\n",
    "        self.attn_weights = torch.bmm(lstm_output, hidden) # attn_weights : [batch_size, n_step]\n",
    "\n",
    "        soft_attn_weights = torch.tanh(self.attn_weights)\n",
    "\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights).squeeze(2)\n",
    "        return context # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=x.permute(0,2,1)\n",
    "        x=self.conv(x)\n",
    "        x=x.permute(0,2,1)\n",
    "        batch_size, seq_len = x.size()[0], x.size()[1]    \n",
    "        \n",
    "        h_0 = torch.randn(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c_0 = torch.randn(self.num_directions * self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # output(batch_size, seq_len, num_directions * hidden_size)\n",
    "        lstm_out, self.hidden_cell = self.lstm(x) \n",
    "        lstm_out=lstm_out.permute(1,0,2)\n",
    "        # print('lstm_out',lstm_out.shape,self.hidden_cell[0].shape,self.hidden_cell[1].shape)\n",
    "        attn_output = self.attention_net(lstm_out,self.hidden_cell[0])\n",
    "        # print('attn_output',attn_output.shape)\n",
    "        pred = self.fc(attn_output.view(-1,self.hidden_size*self.num_directions))  \n",
    "        # print(pred.shape)\n",
    "        # pred = pred[:, -1, :]  \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100 \n",
    "lr=0.004 \n",
    "momentum=0.2 \n",
    "weight_decay=6e-4  \n",
    "in_channels=input_size=5 \n",
    "out_channels=128 \n",
    "hidden_size=64 \n",
    "num_layers=1 \n",
    "batch_size=30 \n",
    "output_size=1 \n",
    "seq_length=7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='BILSTM'\n",
    "model_att=BiLSTM_BIGRU(input_size, hidden_size, num_layers, output_size, batch_size).to(device)\n",
    "loss_function=nn.MSELoss().to(device)\n",
    "optimizer_att = torch.optim.SGD(model_att.parameters(), lr=lr,momentum=momentum, weight_decay=weight_decay)\n",
    "print(model_att)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_train_dic=[] \n",
    "loss_train_dic=[] \n",
    "value_val_dic=[] \n",
    "loss_val_dic=[] \n",
    "path='model_result.txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import copy\n",
    "def train_model(model,optimizer):\n",
    "    model.train()  \n",
    "    print('训练','模型') \n",
    "    min_epochs=0 \n",
    "    best_model=None \n",
    "    min_val_loss=5 \n",
    "    train_loss_list=[]  \n",
    "    val_loss_list=[] \n",
    "    for epoch in tqdm(range(epochs)): \n",
    "        #train\n",
    "        add=0\n",
    "        train_loss=[]  \n",
    "        val_loss=[]  \n",
    "        for (seq,label) in train_loader:     \n",
    "            optimizer.zero_grad()  \n",
    "            seq=seq.to(device)  \n",
    "            label=label.to(device)  \n",
    "            y_pred = model(seq)  \n",
    "            \n",
    "            if(epoch==epochs-1):  \n",
    "                value_train_dic.append(y_pred)  \n",
    "            # print(y_pred.shape,label.shape)\n",
    "            single_loss = loss_function(y_pred, label.reshape(-1,1))   \n",
    "            train_loss.append(single_loss.item())    \n",
    "            single_loss .backward()    \n",
    "            optimizer.step()   \n",
    "\n",
    "        loss_train_dic.append(np.mean(train_loss))   \n",
    "        \n",
    "\n",
    "        #val    \n",
    "        add=0 \n",
    "        t=0\n",
    "\n",
    "        for (seq,label) in test_loader:  \n",
    "            with torch.no_grad():   \n",
    "                seq = seq.to(device)    \n",
    "                label=label.to(device)    \n",
    "                y_pred=model(seq)   \n",
    "                single_loss=loss_function(y_pred,label.reshape(-1,1))    \n",
    "                val_loss.append(single_loss.item())    \n",
    "                \n",
    "                if(epoch==epochs): \n",
    "                    value_val_dic.append(y_pred)    \n",
    "\n",
    "        loss_val_dic.append(np.mean(val_loss))    \n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, np.mean(train_loss), np.mean(val_loss)))\n",
    "\n",
    "        train_loss_list.append(np.mean(train_loss)) \n",
    "        val_loss_list.append(np.mean(val_loss))   \n",
    "        if epoch > min_epochs and np.mean(val_loss) < min_val_loss:   \n",
    "            min_val_loss = np.mean(val_loss)   \n",
    "            best_model = copy.deepcopy(model)   \n",
    "\n",
    "    state = {'models': best_model.state_dict()}   \n",
    "    torch.save(state, path)  \n",
    "    print('----------------------')\n",
    "    return train_loss_list,val_loss_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_list,val_loss_list=train_model(model_att,optimizer_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss曲线\n",
    "plt.figure(figsize=(12,4),dpi=120)  \n",
    "plt.plot(range(epochs),train_loss_list,label=\"loss\") \n",
    "plt.plot(range(epochs),val_loss_list,label=\"val_loss\") \n",
    "plt.title(\"Loss Curve\",fontsize=18) \n",
    "plt.xlabel(\"Epochs\",fontsize=15)\n",
    "plt.ylabel(\"Loss\",fontsize=15)   \n",
    "plt.grid(alpha=0.3) \n",
    "plt.legend()  \n",
    "plt.title('bilstm')   \n",
    "plt.savefig('bilstm-loss.jpg',bbox_inches = 'tight',dpi=600) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=model_att \n",
    "pred=[] \n",
    "y_true=[]  \n",
    "loss=[]   \n",
    "model.load_state_dict(torch.load(path)['models'])  \n",
    "model.eval()   \n",
    "\n",
    "for (seq,target) in tqdm(test_loader):   \n",
    "    seq = seq.to(device)  \n",
    "    target=target.to(device)  \n",
    "\n",
    "    y_true.extend(target.cpu())  \n",
    "    with torch.no_grad():  \n",
    "      # model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "      #                 torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        y_pred=model(seq)  \n",
    "        pred.extend(y_pred.cpu())  \n",
    "\n",
    "    loss.append(loss_function(y_pred, target.reshape(-1,1))) \n",
    "\n",
    "y_true, pred = np.array(y_true), np.array(pred) \n",
    "y_true = y_true.reshape(-1,1)   \n",
    "pred = pred.reshape(-1,1)  \n",
    "print(y_true.shape,pred.shape) \n",
    "y_true=scaler2.inverse_transform(y_true).ravel() \n",
    "pred=scaler2.inverse_transform(pred).ravel()   \n",
    "\n",
    "#保存结果\n",
    "data = {'y_true': y_true,'pred': pred}\n",
    "df = pd.DataFrame(data, index=y_validate_index.ravel()[:len(y_true)])\n",
    "\n",
    "# 保存到Excel\n",
    "df.to_excel('person-BiLSTM预测值和真实值.xlsx')\n",
    "\n",
    "\n",
    "x = [i for i in range(len(y_true))]\n",
    "plt.figure(figsize=(12, 6),dpi=120)   \n",
    "# plt.grid(True)\n",
    "\n",
    "\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],y_true,color=\"red\",label='real value')\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],pred,label='prediction')\n",
    "\n",
    "plt.title('bilstm')   \n",
    "plt.ylabel('demand')  \n",
    "plt.xlabel('date')   \n",
    "\n",
    "plt.legend(loc='upper right',fontsize=15)  \n",
    "plt.savefig('bilstm-true-vs-predict.jpg',bbox_inches = 'tight',dpi=600) \n",
    "\n",
    "print('rmse: ',metrics.mean_squared_error(y_true, pred,squared=False),\n",
    "      'mae: ',metrics.mean_absolute_error(y_true, pred),\n",
    "      'r2: ',metrics.r2_score(y_true, pred)\n",
    "     )\n",
    "result_1=[metrics.mean_squared_error(y_true, pred,squared=False),metrics.mean_absolute_error(y_true, pred),metrics.r2_score(y_true, pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bilstm-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "lr=0.004\n",
    "momentum=0.2\n",
    "weight_decay=6e-4\n",
    "in_channels=input_size=5\n",
    "out_channels=128\n",
    "hidden_size=64\n",
    "num_layers=1\n",
    "batch_size=30\n",
    "output_size=1\n",
    "seq_length=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='BILSTM-ATT'\n",
    "model_att=BILSTM_BIGRU_Attention(input_size, hidden_size, num_layers, output_size, batch_size).to(device)\n",
    "loss_function=nn.MSELoss().to(device)\n",
    "optimizer_att = torch.optim.SGD(model_att.parameters(), lr=lr,momentum=momentum, weight_decay=weight_decay)\n",
    "print(model_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_train_dic=[]\n",
    "loss_train_dic=[]\n",
    "value_val_dic=[]\n",
    "loss_val_dic=[]\n",
    "path='model_result.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_model(model,optimizer):\n",
    "    model.train()\n",
    "    print('训练','模型')\n",
    "    min_epochs=0\n",
    "    best_model=None\n",
    "    min_val_loss=5\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        #train\n",
    "        add=0\n",
    "        train_loss=[]\n",
    "        val_loss=[]\n",
    "        for (seq,label) in train_loader:   \n",
    "            optimizer.zero_grad()\n",
    "            seq=seq.to(device)\n",
    "            label=label.to(device)\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            if(epoch==epochs-1):  #对最后一次epoch的值进行记录\n",
    "                value_train_dic.append(y_pred)\n",
    "            # print(y_pred.shape,label.shape)\n",
    "            single_loss = loss_function(y_pred, label.reshape(-1,1))   \n",
    "            train_loss.append(single_loss.item())\n",
    "            single_loss .backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_train_dic.append(np.mean(train_loss))\n",
    "\n",
    "        #val\n",
    "        add=0 \n",
    "        t=0\n",
    "\n",
    "        for (seq,label) in test_loader:\n",
    "            with torch.no_grad():\n",
    "                seq = seq.to(device)\n",
    "                label=label.to(device)\n",
    "                y_pred=model(seq)\n",
    "                single_loss=loss_function(y_pred,label.reshape(-1,1))\n",
    "                val_loss.append(single_loss.item())\n",
    "\n",
    "                if(epoch==epochs):  #对最后一次epoch的值进行记录\n",
    "                    value_val_dic.append(y_pred)\n",
    "\n",
    "        loss_val_dic.append(np.mean(val_loss))\n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, np.mean(train_loss), np.mean(val_loss)))\n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        val_loss_list.append(np.mean(val_loss))\n",
    "        if epoch > min_epochs and np.mean(val_loss) < min_val_loss:\n",
    "            min_val_loss = np.mean(val_loss)\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    state = {'models': best_model.state_dict()}\n",
    "    torch.save(state, path)\n",
    "    print('----------------------')\n",
    "    return train_loss_list,val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_list,val_loss_list=train_model(model_att,optimizer_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss曲线\n",
    "plt.figure(figsize=(12,4),dpi=120)\n",
    "plt.plot(range(epochs),train_loss_list,label=\"loss\") # Loss curve for training set\n",
    "plt.plot(range(epochs),val_loss_list,label=\"val_loss\") # Loss curve for validation set\n",
    "plt.title(\"Loss Curve\",fontsize=18)\n",
    "plt.xlabel(\"Epochs\",fontsize=15)\n",
    "plt.ylabel(\"Loss\",fontsize=15)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('bilstm-attention')\n",
    "plt.savefig('bilstm-attention-loss.jpg',bbox_inches = 'tight',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=model_att\n",
    "pred=[]\n",
    "y_true=[]\n",
    "loss=[]\n",
    "model.load_state_dict(torch.load(path)['models'])\n",
    "model.eval()\n",
    "\n",
    "for (seq,target) in tqdm(test_loader):\n",
    "    seq = seq.to(device)\n",
    "    target=target.to(device)\n",
    "\n",
    "    y_true.extend(target.cpu())\n",
    "    with torch.no_grad():\n",
    "      # model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "      #                 torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        y_pred=model(seq)\n",
    "        pred.extend(y_pred.cpu())\n",
    "\n",
    "    loss.append(loss_function(y_pred, target.reshape(-1,1)))\n",
    "\n",
    "y_true, pred = np.array(y_true), np.array(pred)\n",
    "y_true = y_true.reshape(-1,1)\n",
    "pred = pred.reshape(-1,1)\n",
    "print(y_true.shape,pred.shape)\n",
    "y_true=scaler2.inverse_transform(y_true).ravel()\n",
    "pred=scaler2.inverse_transform(pred).ravel()\n",
    "\n",
    "\n",
    "#保存结果\n",
    "\n",
    "data = {'y_true': y_true,'pred': pred}\n",
    "df = pd.DataFrame(data, index=y_validate_index.ravel()[:len(y_true)])\n",
    "\n",
    "# 保存到Excel\n",
    "df.to_excel('person-BiLSTM-ATT预测值和真实值.xlsx')\n",
    "\n",
    "x = [i for i in range(len(y_true))]\n",
    "plt.figure(figsize=(12, 6),dpi=120)\n",
    "# plt.grid(True)\n",
    "\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],y_true,color=\"red\",label='real value')\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],pred,label='prediction')\n",
    "\n",
    "plt.title('bilstm-attention')\n",
    "plt.ylabel('demand')\n",
    "plt.xlabel('date')\n",
    "\n",
    "plt.legend(loc='upper right',fontsize=15)\n",
    "plt.savefig('bilstm-attention-true-vs-predict.jpg',bbox_inches = 'tight',dpi=600)\n",
    "print('rmse: ',metrics.mean_squared_error(y_true, pred,squared=False),\n",
    "      'mae: ',metrics.mean_absolute_error(y_true, pred),\n",
    "      'r2: ',metrics.r2_score(y_true, pred)\n",
    "     )\n",
    "result_2=[metrics.mean_squared_error(y_true, pred,squared=False),metrics.mean_absolute_error(y_true, pred),metrics.r2_score(y_true, pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn-bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "lr=0.004\n",
    "momentum=0.2\n",
    "weight_decay=6e-4\n",
    "in_channels=input_size=5\n",
    "out_channels=128\n",
    "hidden_size=64\n",
    "num_layers=1\n",
    "batch_size=30\n",
    "output_size=1\n",
    "seq_length=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_att=CNN_BILSTM(in_channels,out_channels, hidden_size, num_layers, output_size, batch_size,seq_length).to(device)\n",
    "loss_function=nn.MSELoss().to(device)\n",
    "optimizer_att = torch.optim.SGD(model_att.parameters(), lr=lr,momentum=momentum, weight_decay=weight_decay)\n",
    "print(model_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_train_dic=[]\n",
    "loss_train_dic=[]\n",
    "value_val_dic=[]\n",
    "loss_val_dic=[]\n",
    "path='model_result.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_model(model,optimizer):\n",
    "    model.train()\n",
    "    print('训练','模型')\n",
    "    min_epochs=0\n",
    "    best_model=None\n",
    "    min_val_loss=5\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        #train\n",
    "        add=0\n",
    "        train_loss=[]\n",
    "        val_loss=[]\n",
    "        for (seq,label) in train_loader:   \n",
    "            optimizer.zero_grad()\n",
    "            seq=seq.to(device)\n",
    "            label=label.to(device)\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            if(epoch==epochs-1):  \n",
    "                value_train_dic.append(y_pred)\n",
    "            # print(y_pred.shape,label.shape)\n",
    "            single_loss = loss_function(y_pred, label.reshape(-1,1))   \n",
    "            train_loss.append(single_loss.item())\n",
    "            single_loss .backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_train_dic.append(np.mean(train_loss))\n",
    "\n",
    "        #val\n",
    "        add=0 \n",
    "        t=0\n",
    "\n",
    "        for (seq,label) in test_loader:\n",
    "            with torch.no_grad():\n",
    "                seq = seq.to(device)\n",
    "                label=label.to(device)\n",
    "                y_pred=model(seq)\n",
    "                single_loss=loss_function(y_pred,label.reshape(-1,1))\n",
    "                val_loss.append(single_loss.item())\n",
    "\n",
    "                if(epoch==epochs):  #对最后一次epoch的值进行记录\n",
    "                    value_val_dic.append(y_pred)\n",
    "\n",
    "        loss_val_dic.append(np.mean(val_loss))\n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, np.mean(train_loss), np.mean(val_loss)))\n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        val_loss_list.append(np.mean(val_loss))\n",
    "        if epoch > min_epochs and np.mean(val_loss) < min_val_loss:\n",
    "            min_val_loss = np.mean(val_loss)\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    state = {'models': best_model.state_dict()}\n",
    "    torch.save(state, path)\n",
    "    print('----------------------')\n",
    "    return train_loss_list,val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_list,val_loss_list=train_model(model_att,optimizer_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss曲线\n",
    "plt.figure(figsize=(12,4),dpi=120)\n",
    "plt.plot(range(epochs),train_loss_list,label=\"loss\") # Loss curve for training set\n",
    "plt.plot(range(epochs),val_loss_list,label=\"val_loss\") # Loss curve for validation set\n",
    "plt.title(\"Loss Curve\",fontsize=18)\n",
    "plt.xlabel(\"Epochs\",fontsize=15)\n",
    "plt.ylabel(\"Loss\",fontsize=15)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('cnn-bilstm')\n",
    "plt.savefig('cnn-bilstm-loss.jpg',bbox_inches = 'tight',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=model_att\n",
    "pred=[]\n",
    "y_true=[]\n",
    "loss=[]\n",
    "model.load_state_dict(torch.load(path)['models'])\n",
    "model.eval()\n",
    "\n",
    "for (seq,target) in tqdm(test_loader):\n",
    "    seq = seq.to(device)\n",
    "    target=target.to(device)\n",
    "\n",
    "    y_true.extend(target.cpu())\n",
    "    with torch.no_grad():\n",
    "      # model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "      #                 torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        y_pred=model(seq)\n",
    "        pred.extend(y_pred.cpu())\n",
    "\n",
    "    loss.append(loss_function(y_pred, target.reshape(-1,1)))\n",
    "\n",
    "y_true, pred = np.array(y_true), np.array(pred)\n",
    "y_true = y_true.reshape(-1,1)\n",
    "pred = pred.reshape(-1,1)\n",
    "print(y_true.shape,pred.shape)\n",
    "y_true=scaler2.inverse_transform(y_true).ravel()\n",
    "pred=scaler2.inverse_transform(pred).ravel()\n",
    "\n",
    "\n",
    "#保存结果\n",
    "data = {'y_true': y_true,'pred': pred}\n",
    "df = pd.DataFrame(data, index=y_validate_index.ravel()[:len(y_true)])\n",
    "\n",
    "# 保存到Excel\n",
    "df.to_excel('person-CNN-BiLSTM预测值和真实值.xlsx')\n",
    "\n",
    "x = [i for i in range(len(y_true))]\n",
    "plt.figure(figsize=(12, 6),dpi=120)\n",
    "# plt.grid(True)\n",
    "\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],y_true,color=\"red\",label='real value')\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],pred,label='prediction')\n",
    "\n",
    "plt.title('cnn-bilstm')\n",
    "plt.ylabel('demand')\n",
    "plt.xlabel('date')\n",
    "\n",
    "plt.legend(loc='upper right',fontsize=15)\n",
    "plt.savefig('cnn-bilstm-true-vs-predict.jpg',bbox_inches = 'tight',dpi=600)\n",
    "print('rmse: ',metrics.mean_squared_error(y_true, pred,squared=False),\n",
    "      'mae: ',metrics.mean_absolute_error(y_true, pred),\n",
    "      'r2: ',metrics.r2_score(y_true, pred)\n",
    "     )\n",
    "result_5=[metrics.mean_squared_error(y_true, pred,squared=False),metrics.mean_absolute_error(y_true, pred),metrics.r2_score(y_true, pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn-bilstm-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "lr=0.004\n",
    "momentum=0.2\n",
    "weight_decay=6e-4\n",
    "in_channels=input_size=5\n",
    "out_channels=128\n",
    "hidden_size=64\n",
    "num_layers=1\n",
    "batch_size=30\n",
    "output_size=1\n",
    "seq_length=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_att=CNN_BILSTM_Attention(in_channels,out_channels, hidden_size, num_layers, output_size, batch_size,seq_length).to(device)\n",
    "loss_function=nn.MSELoss().to(device)\n",
    "optimizer_att = torch.optim.SGD(model_att.parameters(), lr=lr,momentum=momentum, weight_decay=weight_decay)\n",
    "print(model_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_train_dic=[]\n",
    "loss_train_dic=[]\n",
    "value_val_dic=[]\n",
    "loss_val_dic=[]\n",
    "path='model_result.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     np.random.seed(seed)\n",
    "     random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True\n",
    "# 设置随机数种子\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def train_model(model,optimizer):\n",
    "    model.train()\n",
    "    print('训练','模型')\n",
    "    min_epochs=0\n",
    "    best_model=None\n",
    "    min_val_loss=5\n",
    "    train_loss_list=[]\n",
    "    val_loss_list=[]\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        #train\n",
    "        add=0\n",
    "        train_loss=[]\n",
    "        val_loss=[]\n",
    "        for (seq,label) in train_loader:   \n",
    "            optimizer.zero_grad()\n",
    "            seq=seq.to(device)\n",
    "            label=label.to(device)\n",
    "            y_pred = model(seq)\n",
    "\n",
    "            if(epoch==epochs-1):  #对最后一次epoch的值进行记录\n",
    "                value_train_dic.append(y_pred)\n",
    "            # print(y_pred.shape,label.shape)\n",
    "            single_loss = loss_function(y_pred, label.reshape(-1,1))   \n",
    "            train_loss.append(single_loss.item())\n",
    "            single_loss .backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_train_dic.append(np.mean(train_loss))\n",
    "\n",
    "        #val\n",
    "        add=0 \n",
    "        t=0\n",
    "\n",
    "        for (seq,label) in test_loader:\n",
    "            with torch.no_grad():\n",
    "                seq = seq.to(device)\n",
    "                label=label.to(device)\n",
    "                y_pred=model(seq)\n",
    "                single_loss=loss_function(y_pred,label.reshape(-1,1))\n",
    "                val_loss.append(single_loss.item())\n",
    "\n",
    "                if(epoch==epochs):  \n",
    "                    value_val_dic.append(y_pred)\n",
    "\n",
    "        loss_val_dic.append(np.mean(val_loss))\n",
    "\n",
    "        print('epoch {:03d} train_loss {:.8f} val_loss {:.8f}'.format(epoch, np.mean(train_loss), np.mean(val_loss)))\n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        val_loss_list.append(np.mean(val_loss))\n",
    "        if epoch > min_epochs and np.mean(val_loss) < min_val_loss:\n",
    "            min_val_loss = np.mean(val_loss)\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "    state = {'models': best_model.state_dict()}\n",
    "    torch.save(state, path)\n",
    "    print('----------------------')\n",
    "    return train_loss_list,val_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss_list,val_loss_list=train_model(model_att,optimizer_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss曲线\n",
    "plt.figure(figsize=(12,4),dpi=120)\n",
    "plt.plot(range(epochs),train_loss_list,label=\"loss\") # Loss curve for training set\n",
    "plt.plot(range(epochs),val_loss_list,label=\"val_loss\") # Loss curve for validation set\n",
    "plt.title(\"Loss Curve\",fontsize=18)\n",
    "plt.xlabel(\"Epochs\",fontsize=15)\n",
    "plt.ylabel(\"Loss\",fontsize=15)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()\n",
    "plt.title('cnn-bilstm-attention')\n",
    "plt.savefig('cnn-bilstm-attention-loss.jpg',bbox_inches = 'tight',dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model=model_att\n",
    "pred=[]\n",
    "y_true=[]\n",
    "loss=[]\n",
    "model.load_state_dict(torch.load(path)['models'])\n",
    "model.eval()\n",
    "\n",
    "for (seq,target) in tqdm(test_loader):\n",
    "    seq = seq.to(device)\n",
    "    target=target.to(device)\n",
    "\n",
    "    y_true.extend(target.cpu())\n",
    "    with torch.no_grad():\n",
    "      # model.hidden = (torch.zeros(1, 1, model.hidden_layer_size),\n",
    "      #                 torch.zeros(1, 1, model.hidden_layer_size))\n",
    "        y_pred=model(seq)\n",
    "        pred.extend(y_pred.cpu())\n",
    "\n",
    "    loss.append(loss_function(y_pred, target.reshape(-1,1)))\n",
    "\n",
    "y_true, pred = np.array(y_true), np.array(pred)\n",
    "y_true = y_true.reshape(-1,1)\n",
    "pred = pred.reshape(-1,1)\n",
    "print(y_true.shape,pred.shape)\n",
    "y_true=scaler2.inverse_transform(y_true).ravel()\n",
    "pred=scaler2.inverse_transform(pred).ravel()\n",
    "\n",
    "\n",
    "#保存结果\n",
    "\n",
    "data = {'y_true': y_true,'pred': pred}\n",
    "df = pd.DataFrame(data, index=y_validate_index.ravel()[:len(y_true)])\n",
    "\n",
    "# 保存到Excel\n",
    "df.to_excel('person-CNN-BiLSTM-ATT预测值和真实值.xlsx')\n",
    "\n",
    "x = [i for i in range(len(y_true))]\n",
    "plt.figure(figsize=(12, 6),dpi=120)\n",
    "# plt.grid(True)\n",
    "\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],y_true,color=\"red\",label='real value')\n",
    "plt.plot(y_validate_index.ravel()[:len(y_true)],pred,label='prediction')\n",
    "\n",
    "plt.title('cnn-bilstm-attention')\n",
    "plt.ylabel('demand')\n",
    "plt.xlabel('date')\n",
    "\n",
    "plt.legend(loc='upper right',fontsize=15)\n",
    "plt.savefig('cnn-bilstm-attention-true-vs-predict.jpg',bbox_inches = 'tight',dpi=600)\n",
    "print('rmse: ',metrics.mean_squared_error(y_true, pred,squared=False),\n",
    "      'mae: ',metrics.mean_absolute_error(y_true, pred),\n",
    "      'r2: ',metrics.r2_score(y_true, pred)\n",
    "     )\n",
    "result_6=[metrics.mean_squared_error(y_true, pred,squared=False),metrics.mean_absolute_error(y_true, pred),metrics.r2_score(y_true, pred)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型对比\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame([result_1,result_2,result_3,result_4,result_5,result_6],\n",
    "             columns=['test_rmse','test_mae','test_r2'],\n",
    "             index=['bilstm','bilstm-attention','bigru','bigru-attention','cnn-bilstm','cnn-bilstm-attention']\n",
    "            )\n",
    "result.to_excel('person前6特征的模型对比.xlsx')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False  \n",
    "model=CNN_BILSTM(in_channels,out_channels, hidden_size, num_layers, output_size, batch_size,seq_length).to(device)\n",
    "model.load_state_dict(torch.load('model_result_cnn_bilstm.txt',map_location=torch.device('cpu'))['models'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "data_tensor = torch.tensor(x_test[:100]).to(device)\n",
    "data_tensor = data_tensor.clone().detach().requires_grad_(True)\n",
    "#shap_values = explainer(x)\n",
    "explainer = shap.DeepExplainer(model, data_tensor)\n",
    "shap_values = explainer.shap_values(data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1=pd.DataFrame(x_test[:100:,0,:],columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap import LinearExplainer, KernelExplainer, Explanation\n",
    "shap_values1 = Explanation(shap_values[:,0,:],explainer.expected_value, data=x_test1,feature_names=cols)\n",
    "shap_values1.base_values=np.array([shap_values1.base_values[0] for i in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "shap.summary_plot(shap_values[:,0,:], x_test1, show=False)\n",
    "plt.savefig('summary_plot1.jpg',dpi=600, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_plot \n",
    "shap.summary_plot(shap_values[:,0,:], x_test1,plot_size=(12,6),plot_type=\"bar\",show=False)\n",
    "plt.savefig('summary_plot2.jpg',dpi=600, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
